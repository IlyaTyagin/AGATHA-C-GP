syntax = "proto2";
package agatha;


// This is used to specify the remote location of pubmed articles.
// Defaults to the most recent baseline.
message FtpSource {
  optional string address = 1 [default="ftp.ncbi.nlm.nih.gov"];
  // The remote location to download from
  optional string workdir = 2 [default="pubmed/baseline"];
}

// The ClusterConfig defines the ray cluster connection param. Any cluster-specific deatuls will occur here.
message ClusterConfig {
  // Where to connect to cluster.
  optional string address = 1 [default="localhost"];
  optional int32 port = 2 [default=8786];
  // Fast scratch dir to use for intermediate storage.
  optional string local_scratch = 3 [default="/tmp"];
  optional string shared_scratch = 4;
  // Restarting cluster refreshes all code, but also clears stored datasets.
  optional bool restart = 5 [default=false];
  optional bool run_locally = 6 [default=false];
  optional bool clear_checkpoints = 7 [default=false];
  optional bool disable_checkpoints = 8 [default=false];
}

// The properties of a mongodb database
message DatabaseConfig {
  optional string address = 1;
  optional int32 port = 2 [default=27017];
  // The database name used in mongo
  optional string name = 3 [default="agatha"];
}

message TextParserConfig {
  optional string scispacy_version = 1;
  // May be a path to a bert model, or any of the transformer pretrained models.
  // Example: "./data/scibert" or "bert-base-uncased"
  optional string bert_model = 2 [default="bert-base-uncased"];
  // Discard sentences that are too long/short
  optional int32 min_sentence_len = 3 [default=10];
  optional int32 max_sentence_len = 4 [default=1000];
  // Path to stopwords. One word per line.
  optional string stopword_list = 5;
  // The BERT model will error out if given a too-long sequence.
  // Long sequences will be truncated.
  optional int32 max_sequence_length = 6 [default=500];
}

message NGramConfig {
  // When performing n-gram mining, how long should a potential n-gram be?
  // Set less than j to disable n-gram mining
  optional int32 max_ngram_length = 1 [default=4];
  // Number of times an n-gram must occur before its determined to be real
  optional int32 min_ngram_support = 2 [default=20];
  // Number of times an n-gram must occur in a single partition in order to
  // count.  This is an approximation factor, because otherwise we're going to
  // communicate a ton of bad ngrams.
  optional int32 min_ngram_support_per_partition = 3 [default=5];
  // How much of the text should we look at when mining ngrams?
  optional float ngram_sample_rate = 4 [default=0.33];
}

message KnnConfig {
  optional int32 num_neighbors = 1 [default=100];
  optional int32 num_centroids = 2 [default=2048];
  optional int32 num_probes = 3 [default=16];
  optional int32 num_quantizers = 4 [default=96];
  optional int32 bits_per_quantizer = 5 [default=8];
  // The probability that a randomly selected record is included in the inital
  // training procedure.
  optional float training_probability = 6 [default=0.01];
}

message LdaConfig {
  optional int32 num_topics = 1 [default=20];
  optional int32 random_seed = 2 [default=42];
  optional int32 iterations = 3 [default=50];
  // Remove any word that does not occur at least X times
  optional int32 min_support_count = 4 [default=2];
  // Remove any word that occurs in X proportion of docs.
  optional float max_support_fraction = 5 [default=0.1];
  // Take the top X words per-topic
  optional int32 truncate_size = 7 [default=250];
}

message MlConfig {
  optional int32 batch_size = 1 [default=32];
  optional bool disable_gpu = 2 [default=false];
  // only used for training
  optional int32 num_epochs = 3 [default=10];
  optional float validation_ratio = 4 [default=0.1];
  optional bool single_gpu = 5 [default=false];
  optional float test_ratio = 6 [default=0.2];
  optional float learning_rate = 7 [default=0.002];
  optional int32 steps_per_epoch = 8 [default=10000];
}

message ConstructDebugConfig {
  optional bool enable = 1 [default=false];
  optional float document_sample_rate = 2 [default=0.1];
  optional int32 partition_subset_size = 3 [default=50];
}

message ShortestPathConfig {
  // Number of nodes to download at a time.
  optional int32 node_batch = 1 [default=10];
}

message PretrainedModelConfig {
  // When you add models to the construction process / ml module, add
  // optional params here to notify the construction process.
  optional string sentence_classifier_path = 1;
}

message MySqlConfig {
  optional string address = 1;
  optional string db = 2;
  optional string user = 3;
  optional string password = 4;
}

////////////////////////////////////////////////////////////////////////////////

// Config used to construct the network.
message ConstructConfig {
  // Details about the ray cluster
  optional ClusterConfig cluster = 1;
  optional DatabaseConfig db = 2;
  // Details about the medline distribution to download.
  optional FtpSource ftp = 3;
  optional TextParserConfig parser = 4;
  optional KnnConfig sentence_knn = 5;
  // if set, sample our input by this rate.
  optional MlConfig sys = 7;
  optional ConstructDebugConfig debug = 8;
  optional NGramConfig phrases = 9;
  // If set, we're going to stop the construction process once we checkpoint
  // this value. Used to help prepare data for the ML process.
  optional string stop_after_ckpt = 10;
  optional PretrainedModelConfig pretrained = 11;

  // If set, produce mongo data files, that can  be loaded with mongoimport
  optional bool export_for_mongo = 12 [default=false];
  // Date string in form of YYYY-MM-DD. e.g. 2015-01-01
  // This is the first date NOT included.
  optional string cut_date = 14;
}

// Config used to query the network.
message QueryConfig {
  optional string source = 1;
  optional string target = 2;
  optional string graph_db = 3;
  optional string bow_db = 4;
  optional int32 max_sentences_per_path_elem = 5 [default=2000];
  optional LdaConfig topic_model = 6;
  optional ShortestPathConfig path = 7;
  // Where to store result proto
  optional string result_path = 8 [default="./agatha_query_result.pb"];
  optional bool override = 9 [default=false];
  optional int32 max_degree = 10 [default=1000];
}

////////////////////////////////////////////////////////////////////////////////

message SentenceClassifierConfig {
  // Root to the scratch directory
  optional ClusterConfig cluster = 1;
  optional MlConfig sys = 2;
  optional float test_set_ratio = 3 [default=0.2];
  optional float validation_set_ratio = 4 [default=0.1];
  optional bool force_retrain = 5 [default=false];

  // Path to custom data, used to specify benchmark data that might not be a
  // typical checkpoint. Note, the sentence data will still be loaded from the
  // typical shared_scratch. This just modifies the train/validation/test data.
  optional string custom_data_dir = 6;
  optional bool use_horovod = 7 [default=false];
}

message AbstractGeneratorConfig {
  optional bool debug = 2 [default=false];
  optional ClusterConfig cluster = 1;
  optional MlConfig sys = 3;
  // May be "train" or "evaluate" or "prep"
  optional string mode = 10 [default="train"];
  optional string model_path = 11;
  // The number of word parts to generate when prepping the model
  // Number of sentences to look at when training sentence piece tokenizer
  optional int32 max_tokenizer_sentences = 14 [default=100000];
  optional int32 embedding_dim = 15 [default=256];
  optional int32 text_length = 16 [default=250];
  optional int32 num_attention_heads = 20 [default=8];
  optional int32 hidden_fc_size = 21 [default=1024];
  optional int32 num_encoder_layers = 22 [default=6];
  optional int32 num_decoder_layers = 23 [default=6];

  // These influence the quality of the tokenization
  optional int32 vocab_size = 13 [default=16000];

  // If set, load these weights. Only used in "mode=train"
  optional string restore_from_checkpoint = 26;

  optional int32 num_warmup_steps = 27 [default=100];
  optional bool lowercase = 28 [default=false];
  optional int32 min_mesh_term_support = 29 [default=5];

  optional string tokenizer_data_path = 30;
  optional string extra_data_path = 31;

  //Needed for pytorch distributed
  optional int32 num_nodes = 33;
  optional int32 accumulate_batches = 34 [default=1];
  optional int32 checkpoint_version = 35 [default=1];
  optional float training_fraction = 36 [default=1.0];
  optional float gradient_clip_val = 38 [default=1.0];

  //Needed for writing results to google sheets
  //Path to the secret json file
  optional string gsheets_api_cred = 39;
  optional string gsheets_title = 40 [default="Abstract Generator Results"];

  optional string predicate_spacy_model = 41 [default="en_core_sci_lg"];
  optional string predicate_stopword_list = 42;

  optional int32 trials_per_generation = 43 [default=1];
}
////////////////////////////////////////////////////////////////////////////////

message SemMedDBAddonConfig {
  optional MySqlConfig semmeddb = 1;
  optional string agathadb = 2;

  // This is the edge weight between a sentence and its predicate Remember that
  // the baseline of 1 is the weight between two adjacent sentences. Also
  // remember that edge weights are a measure of "distance" as in larger =
  // weaker.
  optional float sentence_predicate_weight = 3 [default=0.75];

  // This is the edge weight between a umls term and a predicate
  optional float term_predicate_weight = 4 [default=0.75];
}
